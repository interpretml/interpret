{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc27b1-7903-4d36-9198-190923b85d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use exact versions of these in order to preserve RANK ordering better\n",
    "!pip install -U --quiet numpy==1.26.4 pandas==2.2.2 scikit-learn==1.5.1 xgboost==2.1.0 lightgbm==4.5.0 catboost==1.2.5 aplr==10.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489becf-d522-42a0-af6e-ec99c12d6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install interpret if not already installed\n",
    "try:\n",
    "    import interpret\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet interpret-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5674068-d971-49df-b8a1-60bf91441def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install powerlift if not already installed\n",
    "try:\n",
    "    import powerlift\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet powerlift[datasets,postgres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cfe45-d0f1-4951-953e-cafefee1ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_filter(task):\n",
    "    min_samples = 1\n",
    "    max_samples = 1000000000000\n",
    "    min_features = 1\n",
    "    max_features = 1000000000000\n",
    "    if task.scalar_measure(\"n_rows\") < min_samples:\n",
    "        return []\n",
    "    if max_samples < task.scalar_measure(\"n_rows\"):\n",
    "        return []\n",
    "    if task.scalar_measure(\"n_cols\") < min_features:\n",
    "        return []\n",
    "    if max_features < task.scalar_measure(\"n_cols\"):\n",
    "        return []\n",
    "\n",
    "    \n",
    "    if task.origin == \"openml_automl_regression\":\n",
    "        pass  # include in benchmark\n",
    "    elif task.origin == \"openml_automl_classification\":\n",
    "        return []\n",
    "    elif task.origin == \"openml_cc18\":\n",
    "        pass  # include in benchmark\n",
    "    elif task.origin == \"pmlb\":\n",
    "        if task.problem == \"binary\":\n",
    "            return []\n",
    "        elif task.problem == \"multiclass\":\n",
    "            return []\n",
    "        elif task.problem == \"regression\":\n",
    "            return []\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized problem {task.problem}\")\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized origin {task.origin}\")\n",
    "\n",
    "    \n",
    "    exclude_set = set()\n",
    "#    exclude_set = set(['isolet', 'Devnagari-Script', 'CIFAR_10', 'Airlines_DepDelay_10M'])\n",
    "#    exclude_set = set([\n",
    "#        'Fashion-MNIST', 'mfeat-pixel', 'Bioresponse',\n",
    "#        'mfeat-factors', 'isolet', 'cnae-9', \"Internet-Advertisements\",\n",
    "#        'har', 'Devnagari-Script', 'mnist_784', 'CIFAR_10',\n",
    "#        'Airlines_DepDelay_10M',\n",
    "#    ])\n",
    "    if task.name in exclude_set:\n",
    "        return []\n",
    "\n",
    "\n",
    "    # exclude duplicates of a dataset if they appear twice\n",
    "    global global_duplicates\n",
    "    try:\n",
    "        duplicates = global_duplicates\n",
    "    except NameError:\n",
    "        duplicates = set()\n",
    "        global_duplicates = duplicates\n",
    "    key = (task.name, task.scalar_measure(\"n_rows\"), task.scalar_measure(\"n_cols\"))\n",
    "    if key in duplicates:\n",
    "        print(f\"Excluding duplicate: {key}\")\n",
    "        return []\n",
    "    else:\n",
    "        duplicates.add(key)\n",
    "\n",
    "\n",
    "    return [\n",
    "        \"ebm-base\",\n",
    "        \"xgboost-base\",\n",
    "        \"aplr-base\",\n",
    "        # \"lightgbm-base\",\n",
    "        # \"catboost-base\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43ca15-4a20-4622-a877-0a5af322b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_runner(trial):\n",
    "    seed=42\n",
    "    max_interaction_features=1000\n",
    "    ebm_base_params = {}\n",
    "    xgb_base_params = {}\n",
    "    lightgbm_base_params = {}\n",
    "    catboost_base_params = {}\n",
    "    # ebm_base_params = {\"max_rounds\":2, \"interactions\":0}\n",
    "    # xgb_base_params = {\"n_estimators\":1}\n",
    "    # lightgbm_base_params = {\"n_estimators\":1}\n",
    "    # catboost_base_params = {\"n_estimators\":1}\n",
    "\n",
    "    if max_interaction_features < trial.task.scalar_measure(\"n_cols\"):\n",
    "        # TODO: EBMs can crash for now with too many interactions, so limit it until we have better fix\n",
    "        ebm_base_params[\"interactions\"] = 0\n",
    "\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "    from aplr import APLRClassifier, APLRRegressor\n",
    "    from sklearn.metrics import roc_auc_score, root_mean_squared_error, log_loss\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import numpy as np\n",
    "    from time import time\n",
    "    import warnings\n",
    "\n",
    "    X, y, meta = trial.task.data([\"X\", \"y\", \"meta\"])\n",
    "\n",
    "    for col in X.columns:\n",
    "        # catboost doesn't like missing categoricals, so make them a category\n",
    "        col_data = X[col]\n",
    "        if str(col_data.dtype) == \"category\" and col_data.isnull().any():\n",
    "            X[col] = col_data.cat.add_categories('NaN').fillna('NaN')\n",
    "    \n",
    "    categoricals = meta[\"categorical_mask\"]\n",
    "    categorical_ints = [i for i, val in enumerate(categoricals) if val]\n",
    "    \n",
    "    # XGB and EBM already handle this via CategoricalDtype but make it clear\n",
    "    xgb_feature_types = [\"c\" if cat else \"q\" for cat in categoricals]\n",
    "    ebm_feature_types = [\"nominal\" if cat else \"continuous\" for cat in categoricals]\n",
    "\n",
    "    stratification = None\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        # stratification = y\n",
    "        pass  # Re-enable stratification if dataset fails from absent class in train/test sets (PMLB)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=stratification, random_state=seed)\n",
    "\n",
    "    # Build preprocessor\n",
    "    is_cat = meta[\"categorical_mask\"]\n",
    "    cat_cols = [idx for idx in range(X.shape[1]) if is_cat[idx]]\n",
    "    num_cols = [idx for idx in range(X.shape[1]) if not is_cat[idx]]\n",
    "    cat_ohe_step = (\"ohe\", OneHotEncoder(sparse_output=True, handle_unknown=\"ignore\"))\n",
    "    cat_pipe = Pipeline([cat_ohe_step])\n",
    "    num_pipe = Pipeline([(\"identity\", FunctionTransformer())])\n",
    "    transformers = [(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols)]\n",
    "    ct = Pipeline(\n",
    "        [\n",
    "            (\"ct\", ColumnTransformer(transformers=transformers, sparse_threshold=0)),\n",
    "            (\n",
    "                \"missing\",\n",
    "                SimpleImputer(add_indicator=True, strategy=\"most_frequent\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Specify method\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        if trial.method.name == \"ebm-base\":\n",
    "            est = ExplainableBoostingClassifier(feature_types=ebm_feature_types, **ebm_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        elif trial.method.name == \"xgboost-base\":\n",
    "            est = XGBClassifier(enable_categorical=True, feature_types=xgb_feature_types, **xgb_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train, \"verbose\": False}\n",
    "        elif trial.method.name == \"aplr-base\":\n",
    "            est = Pipeline(\n",
    "                [\n",
    "                    (\"ct\", ct),\n",
    "                    (\n",
    "                        \"est\",\n",
    "                        APLRClassifier(m=3000),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            y_train = y_train.astype(str).to_numpy()\n",
    "            y_test = y_test.astype(str).to_numpy()\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        elif trial.method.name == \"lightgbm-base\":\n",
    "            est = LGBMClassifier(verbosity=-1, **lightgbm_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train, \"categorical_feature\": categorical_ints}\n",
    "        elif trial.method.name == \"catboost-base\":\n",
    "            est = CatBoostClassifier(verbose=False, **catboost_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train, \"cat_features\": categorical_ints}\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized method name {trial.method.name}\")\n",
    "\n",
    "        predict_fn = est.predict_proba\n",
    "    elif trial.task.problem == \"regression\":\n",
    "        if trial.method.name == \"ebm-base\":\n",
    "            est = ExplainableBoostingRegressor(feature_types=ebm_feature_types, **ebm_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        elif trial.method.name == \"xgboost-base\":\n",
    "            est = XGBRegressor(enable_categorical=True, feature_types=xgb_feature_types, **xgb_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train, \"verbose\": False}\n",
    "        elif trial.method.name == \"aplr-base\":\n",
    "            est = Pipeline(\n",
    "                [\n",
    "                    (\"ct\", ct),\n",
    "                    (\n",
    "                        \"est\",\n",
    "                        APLRRegressor(m=3000),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        elif trial.method.name == \"lightgbm-base\":\n",
    "            est = LGBMRegressor(verbosity=-1, **lightgbm_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train, \"categorical_feature\": categorical_ints}\n",
    "        elif trial.method.name == \"catboost-base\":\n",
    "            est = CatBoostRegressor(verbose=False, **catboost_base_params)\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train, \"cat_features\": categorical_ints}\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized method name {trial.method.name}\")\n",
    "\n",
    "        predict_fn = est.predict\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized problem {trial.task.problem}\")\n",
    "\n",
    "    global global_counter\n",
    "    try:\n",
    "        global_counter += 1\n",
    "    except NameError:\n",
    "        global_counter = 0\n",
    "    \n",
    "    # Train\n",
    "    print(f\"FIT: {global_counter}, {trial.task.origin}, {trial.task.name}, {trial.method.name}, \", end=\"\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        start_time = time()\n",
    "        est.fit(**fit_params)\n",
    "        elapsed_time = time() - start_time\n",
    "    trial.log(\"fit_time\", elapsed_time)\n",
    "    \n",
    "    # Predict\n",
    "    start_time = time()\n",
    "    predictions = predict_fn(X_test)\n",
    "    elapsed_time = time() - start_time\n",
    "    trial.log(\"predict_time\", elapsed_time)\n",
    "\n",
    "    if trial.task.problem == \"binary\":\n",
    "        predictions = predictions[:,1]\n",
    "\n",
    "        eval_score = roc_auc_score(y_test, predictions)\n",
    "        trial.log(\"auc\", eval_score)\n",
    "\n",
    "        eval_score2 = log_loss(y_test, predictions)\n",
    "        trial.log(\"log_loss\", eval_score2)\n",
    "    elif trial.task.problem == \"multiclass\":\n",
    "        eval_score = roc_auc_score(y_test, predictions, average=\"weighted\", multi_class=\"ovo\")\n",
    "        trial.log(\"multi_auc\", eval_score)\n",
    "\n",
    "        eval_score2 = log_loss(y_test, predictions)\n",
    "        trial.log(\"cross_entropy\", eval_score2)\n",
    "    elif trial.task.problem == \"regression\":\n",
    "        # Use NRMSE-IQR (normalized root mean square error by the interquartile range)\n",
    "        # so that datasets with large predicted values do not dominate the benchmark\n",
    "        # and the range is not sensitive to outliers. The rank is identical to RMSE.\n",
    "        # https://en.wikipedia.org/wiki/Root_mean_square_deviation\n",
    "\n",
    "        # Get quartile_range from the full dataset for consistency across seeds.\n",
    "        q75, q25 = np.percentile(y, [75, 25])\n",
    "        interquartile_range = q75 - q25\n",
    "\n",
    "        eval_score = root_mean_squared_error(y_test, predictions) / interquartile_range\n",
    "        trial.log(\"nrmse\", eval_score)\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized problem {trial.task.problem}\")\n",
    "\n",
    "    print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226e77-753a-4e1c-bb6b-d80156845785",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recreate=False\n",
    "exist_ok=True\n",
    "is_local=True\n",
    "\n",
    "import datetime\n",
    "experiment_name = datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M__\") + \"myexperiment\"\n",
    "print(\"Experiment name: \" + experiment_name)\n",
    "\n",
    "import os\n",
    "if is_local:\n",
    "    conn_str = f\"sqlite:///{os.getcwd()}/powerlift.db\"\n",
    "else:\n",
    "    from azure.identity import AzureCliCredential\n",
    "    credential = AzureCliCredential()\n",
    "    \n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    experiment_config = {\n",
    "        # \"0.6.3\": [\"interpret_core-0.6.3-py3-none-any.whl\"],\n",
    "        \"0.6.2\": [\"interpret_core-0.6.2-py3-none-any.whl\"],\n",
    "    }\n",
    "    TIMEOUT_SEC = 60 * 60 * 6  # 6 hours\n",
    "    conn_str = os.getenv(\"DOCKER_DB_URL\")\n",
    "    azure_tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "    azure_client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    azure_client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "    subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "    resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "\n",
    "from powerlift.bench import retrieve_openml_automl_regression, retrieve_openml_automl_classification, retrieve_openml_cc18, retrieve_catboost_50k, retrieve_pmlb\n",
    "from powerlift.bench import Benchmark, Store, populate_with_datasets\n",
    "from powerlift.executors import LocalMachine, AzureContainerInstance\n",
    "from itertools import chain\n",
    "\n",
    "# Initialize database (if needed).\n",
    "store = Store(conn_str, force_recreate=force_recreate)\n",
    "\n",
    "cache_dir=\"~/.powerlift\"\n",
    "data_retrieval = chain(\n",
    "    retrieve_openml_automl_regression(cache_dir=cache_dir),\n",
    "    # retrieve_openml_automl_classification(cache_dir=cache_dir),\n",
    "    retrieve_openml_cc18(cache_dir=cache_dir),\n",
    "    # retrieve_catboost_50k(cache_dir=cache_dir),\n",
    "    # retrieve_pmlb(cache_dir=cache_dir),\n",
    ")\n",
    "\n",
    "# This downloads datasets once and feeds into the database.\n",
    "populate_with_datasets(store, data_retrieval, exist_ok=exist_ok)\n",
    "\n",
    "# Run experiment\n",
    "benchmark = Benchmark(store, name=experiment_name)\n",
    "\n",
    "if is_local:\n",
    "    benchmark.run(trial_runner, trial_filter, executor=LocalMachine(store, debug_mode=True))\n",
    "else:\n",
    "    for name, wheel_filepaths in experiment_config.items():\n",
    "        executor = AzureContainerInstance(\n",
    "            store, azure_tenant_id, azure_client_id, azure_client_secret, subscription_id, resource_group, credential,\n",
    "            image=\"benchregistry.azurecr.io/powerlift:0.1.9\",\n",
    "            wheel_filepaths=wheel_filepaths,\n",
    "            n_running_containers=200, num_cores=4, mem_size_gb=16, delete_group_container_on_complete=True\n",
    "        )\n",
    "        benchmark.run(trial_runner, trial_filter, timeout=TIMEOUT_SEC, executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5745059-f747-4c02-8720-0b2d49aa4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.wait_until_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5745059-f747-4c02-8720-0b2d49aa4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-establish connection\n",
    "benchmark = Benchmark(conn_str, name=experiment_name)\n",
    "\n",
    "results_df = benchmark.results()\n",
    "results_df.to_csv(f\"results-{experiment_name}.csv\", index=None)\n",
    "\n",
    "status_df = benchmark.status()\n",
    "for errmsg in status_df[\"errmsg\"]:\n",
    "    if errmsg is not None:\n",
    "        print(\"ERROR: \" + str(errmsg))\n",
    "print(status_df['status'].value_counts().to_string(index=True, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716b066-7d8b-4163-8d5d-a1b7f0274dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reload if analyzing later\n",
    "results_df = pd.read_csv(f\"results-{experiment_name}.csv\")\n",
    "\n",
    "averages = results_df.groupby(['method', 'name'])['num_val'].mean().unstack().reset_index()\n",
    "\n",
    "metric_ranks = results_df.pivot_table('num_val', ['task', 'name'], 'method')\n",
    "metric_ranks = metric_ranks.rank(axis=1, ascending=True, method='min')\n",
    "metric_ranks = metric_ranks.groupby('name').mean().transpose()\n",
    "metric_ranks.columns = [f\"{col}_RANK\" for col in metric_ranks.columns]\n",
    "metric_ranks = metric_ranks.reset_index()\n",
    "\n",
    "overall_rank = results_df[results_df['name'].isin(['log_loss', 'cross_entropy', 'nrmse'])]\n",
    "overall_rank = overall_rank.pivot_table('num_val', 'task', 'method')\n",
    "overall_rank = overall_rank.rank(axis=1, ascending=True, method='min')\n",
    "overall_rank = overall_rank.mean()\n",
    "overall_rank = overall_rank.to_frame(name='RANK').reset_index()\n",
    "\n",
    "desired_columns = ['method', 'RANK', 'auc', 'multi_auc', 'nrmse', 'log_loss_RANK', 'cross_entropy_RANK', 'nrmse_RANK', 'fit_time', 'predict_time']\n",
    "combined_df = averages.merge(metric_ranks, on='method').merge(overall_rank, on='method')\n",
    "combined_df = combined_df.sort_values(by='RANK')\n",
    "combined_df = combined_df.reindex(columns=desired_columns)\n",
    "\n",
    "print(combined_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2df600-d3e0-430e-8dda-836c09825987",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns = ['method', 'RANK', 'auc', 'multi_auc', 'nrmse', 'log_loss', 'cross_entropy', 'fit_time', 'predict_time']\n",
    "row_order = combined_df[\"method\"]\n",
    "\n",
    "counts = results_df.groupby(['method', 'name']).size().unstack()\n",
    "counts = counts.reindex(row_order, axis=0).reset_index()\n",
    "counts['RANK'] = 0\n",
    "if 'log_loss' in counts.columns:\n",
    "    counts['RANK'] += counts['log_loss']\n",
    "if 'cross_entropy' in counts.columns:\n",
    "    counts['RANK'] += counts['cross_entropy']\n",
    "if 'nrmse' in counts.columns:\n",
    "    counts['RANK'] += counts['nrmse']\n",
    "counts = counts.reindex(columns=desired_columns)\n",
    "print(counts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e78875-935d-4fcd-847b-307be385b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_times = results_df[results_df['name'] == 'fit_time']\n",
    "fit_times = fit_times.pivot_table('num_val', 'task', 'method')\n",
    "fit_times = fit_times.dropna()\n",
    "fit_times[\"ratios\"] = fit_times['ebm-base'] / fit_times['xgboost-base']\n",
    "import numpy as np\n",
    "fit_times_deciles = np.percentile(fit_times[\"ratios\"], [90, 80, 70, 60, 50, 40, 30, 20, 10])\n",
    "fit_times_deciles = [f\"{decile:.2f}  \" for decile in fit_times_deciles]\n",
    "max_ratio= fit_times[\"ratios\"].max()\n",
    "min_ratio= fit_times[\"ratios\"].min()\n",
    "print(\"fit time ratio deciles:\")\n",
    "print(*fit_times_deciles)\n",
    "print(f\"max: {max_ratio:.2f}\")\n",
    "print(f\"min: {min_ratio:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
